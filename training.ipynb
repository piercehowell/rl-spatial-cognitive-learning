{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_gridverse\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "#Setup environment\n",
    "\n",
    "class FlattenObservationWrapper(gym.ObservationWrapper):\n",
    "\tdef __init__(self, env):\n",
    "\t\tsuper().__init__(env)\n",
    "\t\ttotal_size = sum(np.prod(env.observation_space.spaces[key].shape) for key in env.observation_space.spaces)\n",
    "\t\tself.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(total_size,), dtype=np.float32)\n",
    "\n",
    "\tdef observation(self, observation):\n",
    "\t\t# Flatten each part of the observation and then concatenate\n",
    "\t\tflattened_obs = np.concatenate([observation[key].flatten() for key in observation])\n",
    "\t\treturn flattened_obs\n",
    "\t\t\n",
    "def make_env():\n",
    "\tenv= gym.make(\"GV-FourRooms-9x9-v0\")\n",
    "\tenv = FlattenObservationWrapper(env)\n",
    "\treturn env\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "#num_envs = 4 \n",
    "#env = SubprocVecEnv([make_env for _ in range(num_envs)],start_method='spawn')\n",
    "#num_envs = 8  # Number of parallel environments\n",
    "#env = DummyVecEnv([make_env for i in range(num_envs)])\n",
    "\n",
    "env = make_env()\n",
    "#env = FlattenObservationWrapper(env)\n",
    "#model = PPO(\"MultiInputPolicy\", env,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39mtotal_timesteps, callback\u001b[39m=\u001b[39m[checkpoint_callback])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(env, total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m2000000\u001b[39;49m)\n",
      "\u001b[1;32m/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m CheckpointCallback(save_freq\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m, save_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/kurt/Documents/HML/models/\u001b[39m\u001b[39m'\u001b[39m, name_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mppo_model_9x9_custom_reset\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Evaluation and logging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#eval_callback = EvalCallback(env, best_model_save_path='./models/', log_path='./logs/', eval_freq=50000)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps, callback\u001b[39m=\u001b[39;49m[checkpoint_callback])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22706c6569616465732e63632e6761746563682e656475222c2275736572223a226477616c6b6572686f77656c6c33227d/home/dwalkerhowell3/rl_spatial_nav/rl-spatial-cognitive-learning/training.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/rl_spatial_cog/lib/python3.10/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:458\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    448\u001b[0m     \u001b[39mself\u001b[39m: SelfRecurrentPPO,\n\u001b[1;32m    449\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfRecurrentPPO:\n\u001b[1;32m    456\u001b[0m     iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 458\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    459\u001b[0m         total_timesteps,\n\u001b[1;32m    460\u001b[0m         callback,\n\u001b[1;32m    461\u001b[0m         reset_num_timesteps,\n\u001b[1;32m    462\u001b[0m         tb_log_name,\n\u001b[1;32m    463\u001b[0m         progress_bar,\n\u001b[1;32m    464\u001b[0m     )\n\u001b[1;32m    466\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    468\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/anaconda3/envs/rl_spatial_cog/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m reset_num_timesteps \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    425\u001b[0m     \u001b[39m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rl_spatial_cog/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m     76\u001b[0m     maybe_options \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options[env_idx]} \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options[env_idx] \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m---> 77\u001b[0m     obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset(seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_seeds[env_idx], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmaybe_options)\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     79\u001b[0m \u001b[39m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rl_spatial_cog/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected you to pass keyword argument \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m into reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_reset_info[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl_spatial_cog/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:235\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m options \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     warn(\n\u001b[1;32m    232\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[39m{\u001b[39;00moptions\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 235\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl_spatial_cog/lib/python3.10/site-packages/gym/core.py:379\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    378\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     obs, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    380\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(obs), info\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#Training function\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "#model.learn(total_timesteps=1800000)\n",
    "\n",
    "def train_model(env, total_timesteps):\n",
    "    model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=2, device='cuda',n_steps=2048,tensorboard_log=\"/home/kurt/Documents/HML/ppo_tensorboard/\")\n",
    "\n",
    "    # Save the model every 100k steps\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=100000, save_path='/home/kurt/Documents/HML/models/', name_prefix='ppo_model_9x9_custom_reset')\n",
    "\n",
    "    # Evaluation and logging\n",
    "    #eval_callback = EvalCallback(env, best_model_save_path='./models/', log_path='./logs/', eval_freq=50000)\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps, callback=[checkpoint_callback])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(env, total_timesteps=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 5.3600001353770494, Std reward: 0.3907044812480032\n"
     ]
    }
   ],
   "source": [
    "#Test and save full trained model\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=20)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "model.save(\"ppo_gridworld_9x9_raytracing_1_8M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kurt/Documents/HML/rl-spatial-cognitive-learning/training.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kurt/Documents/HML/rl-spatial-cognitive-learning/training.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kurt/Documents/HML/rl-spatial-cognitive-learning/training.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     action, lstm_states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs, state\u001b[39m=\u001b[39mlstm_states, episode_start\u001b[39m=\u001b[39mepisode_starts, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kurt/Documents/HML/rl-spatial-cognitive-learning/training.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     obs, rewards, dones, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kurt/Documents/HML/rl-spatial-cognitive-learning/training.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     episode_starts \u001b[39m=\u001b[39m dones\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kurt/Documents/HML/rl-spatial-cognitive-learning/training.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    324\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym_gridverse/gym.py:133\u001b[0m, in \u001b[0;36mGymEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs the environment dynamics for one timestep.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m    Tuple[Dict[str, numpy.ndarray], float, bool, Dict]: (observation, reward, terminal, info dictionary)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m action_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mouter_env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mint_to_action(action)\n\u001b[0;32m--> 133\u001b[0m reward, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mouter_env\u001b[39m.\u001b[39;49mstep(action_)\n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation, reward, done, {}\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym_gridverse/outer_env.py:55\u001b[0m, in \u001b[0;36mOuterEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Action) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m]:\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Runs the dynamics for one timestep, and returns reward and done flag\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m        Tuple[float, bool]: (reward, terminality)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym_gridverse/envs/inner_env.py:80\u001b[0m, in \u001b[0;36mInnerEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Action) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m]:\n\u001b[1;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Runs the dynamics for one timestep, and returns reward and done flag\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[39m    Internally calls :py:meth:`functional_step` to update the state;  also\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m        Tuple[float, bool]: reward and terminal\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state, reward, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunctional_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, action)\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observation \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m reward, done\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym_gridverse/envs/gridworld.py:80\u001b[0m, in \u001b[0;36mGridWorld.functional_step\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action):\n\u001b[1;32m     78\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39maction \u001b[39m\u001b[39m{action}\u001b[39;00m\u001b[39m does not satisfy action-space\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m next_state \u001b[39m=\u001b[39m transition_with_copy(\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transition_function,\n\u001b[1;32m     82\u001b[0m     state,\n\u001b[1;32m     83\u001b[0m     action,\n\u001b[1;32m     84\u001b[0m     rng\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m gv_debug() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_space\u001b[39m.\u001b[39mcontains(next_state):\n\u001b[1;32m     88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mnext_state does not satisfy state_space\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym_gridverse/envs/transition_functions.py:470\u001b[0m, in \u001b[0;36mtransition_with_copy\u001b[0;34m(transition_function, state, action, rng)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransition_with_copy\u001b[39m(\n\u001b[1;32m    450\u001b[0m     transition_function: TransitionFunction,\n\u001b[1;32m    451\u001b[0m     state: State,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     rng: Optional[rnd\u001b[39m.\u001b[39mGenerator] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m State:\n\u001b[1;32m    456\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utility to perform a non-in-place version of a transition function.\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \n\u001b[1;32m    458\u001b[0m \u001b[39m    NOTE:  This is *not* a transition function (transition functions are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m        State:\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m     next_state \u001b[39m=\u001b[39m fast_copy(state)\n\u001b[1;32m    471\u001b[0m     transition_function(next_state, action, rng\u001b[39m=\u001b[39mrng)\n\u001b[1;32m    472\u001b[0m     \u001b[39mreturn\u001b[39;00m next_state\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/site-packages/gym_gridverse/utils/fast_copy.py:10\u001b[0m, in \u001b[0;36mfast_copy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfast_copy\u001b[39m(x: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m      9\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"returns a deep copy of a generic python object, faster than deepcopy\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mloads(pickle\u001b[39m.\u001b[39mdumps(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/hml_gridverse/lib/python3.11/enum.py:1227\u001b[0m, in \u001b[0;36mEnum.__reduce_ex__\u001b[0;34m(self, proto)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_)\n\u001b[0;32m-> 1227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__reduce_ex__\u001b[39m(\u001b[39mself\u001b[39m, proto):\n\u001b[1;32m   1228\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_, )\n\u001b[1;32m   1230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__deepcopy__\u001b[39m(\u001b[39mself\u001b[39m,memo):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Visualize agent\n",
    "model=RecurrentPPO.load(\"/home/kurt/Documents/HML/models/ppo_model_9x9_custom_reset_2000000_steps\")\n",
    "env= gym.make(\"GV-FourRooms-9x9-v0\")\n",
    "env = FlattenObservationWrapper(env)\n",
    "obs = env.reset()\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "while True:\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    episode_starts = dones\n",
    "    env.render()\n",
    "    if dones:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (lstm_actor): LSTM(123, 256)\n",
       "  (lstm_critic): LSTM(123, 256)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Agent_1_Goal_1', 'empty', 'rooms', 'dynamic_obstacles', 'keydoor', 'crossing', 'teleport', 'memory', 'memory_rooms', 'rooms_det_loc'])\n",
      "dict_keys(['Agent_1_Goal_1', 'empty', 'rooms', 'dynamic_obstacles', 'keydoor', 'crossing', 'teleport', 'memory', 'memory_rooms', 'rooms_det_loc'])\n"
     ]
    }
   ],
   "source": [
    "#from gym_gridverse.envs.reset_functions import reset_function_registry # Adjust the import path\n",
    "\n",
    "# Access the data attribute\n",
    "#registry_data = reset_function_registry.data\n",
    "\n",
    "# Interact with the registry data\n",
    "# e.g., print all registered function names\n",
    "#print(registry_data.keys())\n",
    "#if \"rooms_det_loc\" in registry_data:\n",
    "#    del registry_data[\"rooms_det_loc\"]\n",
    "#print(registry_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hml_gridverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
